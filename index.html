<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hanvit Cho</title>

    <meta name="author" content="Hanvit Cho">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hanvit Cho
                </p>
                <p> I am a Master's student in Mechanical Engineering at Stanford University and a researcher at the Stanford Vision and Learning Lab. 
                    My current work focuses on the development of intelligent robots capable of performing everyday tasks through brain-robot interfaces (BRIs), using neural signal decoding and advanced machine learning techniques. 
                    I am passionate about advancing robotic systems that can seamlessly collaborate with humans to enhance autonomy and task efficiency. </p>
                <p> Previously, I was a robotics intern at <a href="https://www.centrilliontech.com/">Centrillion Technology</a>, where I applied bimanual mobile manipulation techniques to automate manufacturing processes. 
                    I also earned my B.S. in Mechanical Engineering at the University at Buffalo, where I contributed to advanced robotics research and developed a solar navigation system for street signs. </p>
                <p> My long-term goal is to merge my engineering expertise with robotics to create innovative, robust, and socially impactful systems that improve the quality of life and expand the possibilities for human-robot collaboration. </p>
                <p style="text-align:center">&nbsp;|&nbsp;
                  <a href="mailto:hvcho74@stanford.edu">Email</a> &nbsp;|&nbsp;
                  <a href="data/Resume_HanvitCho.pdf">Résumé</a> &nbsp;|&nbsp;
                  <a href="https://www.linkedin.com/in/hanvit-cho-882a52214">LinkedIn</a>&nbsp;|&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="image/HanvitCho.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="image/HanvitCho.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


<!-------------------- Work Experience ----------------------->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;padding-bottom:0px;width:100%;vertical-align:middle">
                <h2><b>Work Expereince</b></h2>
                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="aloha_stop()" onmouseover="aloha_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='aloha_image'><video  width=100% muted autoplay loop>
          <source src="image/aloha.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/aloha.png' width=100%>
        </div>
        <script type="text/javascript">
          function aloha_start() {
            document.getElementById('aloha_image').style.opacity = "1";
          }

          function aloha_stop() {
            document.getElementById('aloha_image').style.opacity = "0";
          }
          aloha_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.centrilliontech.com/">
          <span class="papertitle">Centrillion Technology Inc.</span>
        </a>
        <br> Robotics Internship
        <br>
        <em>June 2024 ~ September 2024</em>
        <br>
        <a href="data/aloha_report.pdf">Report Presentation</a>
        <p></p>
        <p>
          - Applied Trossen Robotics' Mobile ALOHA system (Bimanual Mobile Manipulation) to automate complex manufacturing processes, utilizing imitation learning techniques to enable robots to accurately replicate human actions in dynamic environments.
        </p><p>
          - Conducted extensive experimentation with various policy algorithms (ACT, Diffusion), across diverse simulation and real-world environments, to optimize robotic performance and improve adaptability in different task scenarios.
        </p>
      </td>
    </tr>


</tbody></table>
          

<!-------------------- Research Experience ----------------------->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;padding-bottom:0px;">
                <h2><b>Research Experience</b></h2>
                <p>
                  My research interests center on robotics, particularly in developing advanced robotic controllers for manufacturing automation, medical applications, and biomechanics enhancement through exoskeleton technology.
                </p>
              </td>
            </tr>
          </tbody></table>





          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr onmouseout="noir_stop()" onmouseover="noir_start()">
              <td style="padding:20px;width:25%;vertical-align:middle;">
                <div class="one">
                  <div class="two" id='noir_image'><video  width=120% muted autoplay loop>
                  <source src="image/noir.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='image/noir.png' width=100%>
                </div>
                <script type="text/javascript">
                  function noir_start() {
                    document.getElementById('noir_image').style.opacity = "1";
                  }
        
                  function noir_stop() {
                    document.getElementById('noir_image').style.opacity = "0";
                  }
                  noir_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://svl.stanford.edu/">
                  <span class="papertitle">Stanford Vision and Learning Lab</span>
                </a>
                <br> Stanford Univerisity
                <br>
                <em>June 2024 ~ Present</em>
                <br>Advisor: Ruohan Zhang
                <br>
                <a href="data/corl_paper.pdf">2024 Workshop CoRoboLearn Paper
                </a>
                <p></p>
                <p>
                  - Contributed to projects integrating advanced machine learning models with brain-computer interface (BCI) systems for robotics.
                </p><p>
                  - Collaborated with researchers to apply findings in real-world scenarios, emphasizing the seamless interaction between human intentions and robotic execution.
                </p><p>
                  - Focused on enhancing human-robot interaction using EEG signals by developing efficient algorithms for improved brain signal decoding.
                </p><p>
                  - Minimized user training time and created intuitive applications for robots to perform everyday tasks with minimal oversight.
                </p>
              </td>
            </tr>



          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr onmouseout="sali_stop()" onmouseover="sali_start()">
              <td style="padding:20px;width:25%;vertical-align:middle;">
                <div class="one">
                  <div class="two" id='sali_image'><video  width=120% muted autoplay loop>
                  <source src="image/sali.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='image/sali.png' width=100%>
                </div>
                <script type="text/javascript">
                  function sali_start() {
                    document.getElementById('sali_image').style.opacity = "1";
                  }
        
                  function sali_stop() {
                    document.getElementById('sali_image').style.opacity = "0";
                  }
                  sali_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://sr.stanford.edu/">
                  <span class="papertitle">Salisbury Robotics Lab</span>
                </a>
                <br> Stanford Univerisity
                <br>
                <em>March 2024 ~ June 2024</em>
                <br>Advisor: J. Kenneth Salisbury
                <br>
                <a href="data/Sali_Report.pdf">Report Paper</a>
                <p></p>
                <p>
                  - Focused on creating a robotic system capable of performing critical tasks that require touch-based diagnosis, such as applying pressure and stabilizing limbs.
                </p><p>
                  - Implemented a compliance controller for maintaining contact with dynamic objects and a diagnosis mode to palpate patients and record stiffness data, using the Kinova Gen 3 robot arm equipped with Bota SensOne force sensor and Haply Inverse-3 haptic device.
                </p><p>
                  - Demonstrated the system's ability to maintain consistent contact force, detect varying stiffness levels, and provide accurate haptic feedback. This includes adapting to patient movements without causing harm and enabling detailed tissue stiffness assessment, crucial for detecting abnormalities like tumors.
                </p>
              </td>
            </tr>


            <tr onmouseout="charm_stop()" onmouseover="charm_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='charm_image'><video  width=100% muted autoplay loop>
                  <source src="image/charm.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='image/charm.png' width=100%>
                </div>
                <script type="text/javascript">
                  function charm_start() {
                    document.getElementById('charm_image').style.opacity = "1";
                  }
                  function charm_stop() {
                    document.getElementById('charm_image').style.opacity = "0";
                  }
                  charm_stop()
                </script>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://charm.stanford.edu/Main/HomePage">
                    <span class="papertitle">Collaborative Haptics and Robotics in Medicine Lab</span>
                  </a>
                  <br> Stanford Univerisity
                  <br>
                  <em>March 2024 ~ June 2024</em>
                  <br>Advisor: Allison Okamura
                  <br>
                  <a href="data/charm_Report.pdf">Report Slides</a>
                  <p></p>
                  <p>
                    - Vine robots can navigate tight spaces and complex environments by extending their flexible, tube-like bodies.
                  </p><p>
                    - Developed a base station capable of maintaining up to 15 PSI to facilitate the robot's extension. 
                    Also, engineered new sealing methods with gaskets and thread sealants to ensure an air-tight system.
                  </p><p>
                    - Redesigned clamping and closure systems to accommodate various diameters, simplifying operations.
                  </p>
                </td>
              </tr>





          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr onmouseout="bio_stop()" onmouseover="bio_start()">
              <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
                <div class="one">
                  <div class="two" id='bio_image'><video  width=100% muted autoplay loop>
                  <source src="image/bio.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='image/bio.png' width=100%>
                </div>
                <script type="text/javascript">
                  function bio_start() {
                    document.getElementById('bio_image').style.opacity = "1";
                  }
        
                  function bio_stop() {
                    document.getElementById('bio_image').style.opacity = "0";
                  }
                  bio_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://biomechatronics.stanford.edu/">
                  <span class="papertitle">Stanford Biomechatronics Lab</span>
                </a>
                <br> Stanford Univerisity
                <br>
                <em>September 2023 ~ May 2024</em>
                <br>Advisor: Steve Collins
                <br>
                <p></p>
                <p>
                  - Focused on developing exoskeleton technology to assess and improve human walking balance. 
                </p><p>
                  - Tested human-subjected gait data using various balance metrics to identify balanced gait patterns. 
                </p><p>
                  - Investigated methods for controlling wearable exoskeleton controllers, which could enhance the integration of these devices.
                </p>
              </td>
            </tr>



          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr onmouseout="adams_stop()" onmouseover="adams_start()">
              <td style="padding:20px;width:25%;height:75%;vertical-align:top;">
                <div class="one">
                  <div class="two" id='adams_image'><video  width=100% muted autoplay loop>
                  <source src="image/adams.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='image/adams.png' width=100%>
                </div>
                <script type="text/javascript">
                  function adams_start() {
                    document.getElementById('adams_image').style.opacity = "1";
                  }
        
                  function adams_stop() {
                    document.getElementById('adams_image').style.opacity = "0";
                  }
                  adams_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://adams.eng.buffalo.edu/">
                  <span class="papertitle">Adaptive Design Algorithms, Models & Systems Lab</span>
                </a>
                <br> University at Buffalo
                <br>
                <em>February 2022 ~ May 2023</em>
                <br>Advisor: Souma Chowdhury
                <br>
                <a href="data/Poster_Presentation.pdf"> Poster Presentation</a>
                <p></p>
                <p>
                  -	Programmed ground robots (e-puck2) and aerial robots (crazy-fly) using C++ and Python. 
                </p>
                <p>
                  - Did experiments in the motion capture lab using Vicon Tracker to control the swarm robots at the same time. Could find some packages to move the swarm bots easily and learn how to conduct physical experiments in which dozens of robots jointly search, investigate, or deliver goods by using ROS.
                </p>
                <p>
                  -	Utilized various packages to reduce the delay in communication between robots, computers, and Vicon systems so that I could control the robot more immediately.  
                </p>
              </td>
            </tr>
          </tbody></table>



<!-------------------- Pulblished Papers ----------------------->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;padding-bottom:0px;width:100%;vertical-align:middle">
                <h2><b>Published Papers</b></h2>
                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="noirv2_stop()" onmouseover="noirv2_start()">
              <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
                <div class="one">
                  <div class="two" id='noirv2_image'><video  width=100% muted autoplay loop>
                  <source src="image/noirv2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='image/noirv2.png' width=100%>
                </div>
                <script type="text/javascript">
                  function noirv2_start() {
                    document.getElementById('noirv2_image').style.opacity = "1";
                  }
        
                  function noirv2_stop() {
                    document.getElementById('noirv2_image').style.opacity = "0";
                  }
                  noirv2_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=V3fCUY88Nb">
                  <span class="papertitle">NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities</span>
                </a>
                <br>Tasha Kim, Yingke Wang, <strong>Hanvit Cho</strong>, Alex Hodges
                <br>
                <em>CoRL 2024 Workshop CoroboLearn</em>
                <br>
                <a href="data/noirv2_slides.pdf">Slides</a>
                <p></p>
                <p>
                  The development of an enhanced brain-robot interface (NOIR 2.0) utilizing non-invasive EEG signals to control robots for everyday tasks. The system improves human-robot collaboration by integrating faster brain decoding algorithms, few-shot learning, and one-shot skill parameter prediction, reducing task completion time and human involvement.
                </p>
              </td>
            </tr>



    <tr onmouseout="uav_stop()" onmouseover="uav_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='uav_image'><video  width=100% muted autoplay loop>
          <source src="image/UAV.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/UAV.png' width=100%>
        </div>
        <script type="text/javascript">
          function uav_start() {
            document.getElementById('uav_image').style.opacity = "1";
          }

          function uav_stop() {
            document.getElementById('uav_image').style.opacity = "0";
          }
          uav_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2308.09075">
          <span class="papertitle">Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning</span>
        </a>
        <br>Prajit KrisshnaKumar, Jhoel Witter, Steve Paul, <strong>Hanvit Cho</strong>, Karthik Dantu, Souma Chowdhury
        <br>
        <em>IROS</em>, 2023
        <br>
        <a href="https://buffalo.app.box.com/s/99d4af5ptzj70oktq7l5kglk5ymwuiem">BIB</a>
        <p></p>
        <p>
        The development of a decision support system for air traffic management at Urban Air Mobility vertiports using graph learning.
        Test the decision algorithm using E-pucks to evaluate its efficiency in a real-world setting.
        </p>
      </td>
    </tr>


    <tr onmouseout="hsi_stop()" onmouseover="hsi_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='hsi_image'><video  width=100% muted autoplay loop>
          <source src="image/HSI.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/HSI.png' width=100%>
        </div>
        <script type="text/javascript">
          function hsi_start() {
            document.getElementById('hsi_image').style.opacity = "1";
          }

          function hsi_stop() {
            document.getElementById('hsi_image').style.opacity = "0";
          }
          hsi_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2023/87295/1170354">
          <span class="papertitle">Framework for Analyzing Human Cognition in Operationally-Relevant Human Swarm Interaction</span>
        </a>
        <br>
				Joseph P. Distefano, <strong>Hanvit Cho</strong>, Prajit Krisshnakumar, Souma Chowdhury, Ehsan Esfahani
        <br>
        <em>ASME</em>, 2023
        <br>
        <p></p>
        <p>
        The development and use of a virtual environment to analyze human cognition in operationally relevant human-swarm interaction, focusing on how different conditions impact cognitive states and decision-making.
        </p>
      </td>
    </tr>
</tbody></table>
          

<!-------------------- Footer ----------------------->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template is from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>




        </td>
      </tr>
    </table>
  </body>
</html>
